{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Bluetooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "serverMACAddress = '98:D3:71:FD:A9:6F'  # Put your HC-05 address here\n",
    "port = 1  # Match the setting on the HC-05 module\n",
    "s = socket.socket(socket.AF_BLUETOOTH, socket.SOCK_STREAM, socket.BTPROTO_RFCOMM)\n",
    "s.connect((serverMACAddress,port))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Type something...\n"
     ]
    }
   ],
   "source": [
    "print(\"Connected. Type something...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_bl(text):\n",
    "    s.send(bytes(text, 'UTF-8'))\n",
    "    s.send(bytes(text, 'UTF-8'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech-to-text API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_com(MyText):\n",
    "    if MyText=='right':\n",
    "        send_bl('R2')\n",
    "        send_bl('R2')\n",
    "\n",
    "    elif MyText=='left' or MyText=='lyft':\n",
    "        send_bl('L2')\n",
    "        send_bl('L2')\n",
    "    elif MyText=='go' or MyText=='google':\n",
    "        send_bl('F2')\n",
    "        send_bl('F2')\n",
    "    elif MyText=='backward' or MyText=='back':\n",
    "        send_bl('B2')\n",
    "        send_bl('B2')\n",
    "    elif MyText=='stop':\n",
    "        send_bl('S')\n",
    "        send_bl('S')\n",
    "\n",
    "    else:\n",
    "        send_bl('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to translate\n",
    "# speech to text and text to speech\n",
    "\n",
    "\n",
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "import time\n",
    "\n",
    "# Initialize the recognizer\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# # Function to convert text to\n",
    "# # speech\n",
    "# def SpeakText(command):\n",
    "\t\n",
    "# \t# Initialize the engine\n",
    "# \tengine = pyttsx3.init()\n",
    "# \tengine.say(command)\n",
    "# \tengine.runAndWait()\n",
    "\t\n",
    "\t\n",
    "# # Loop infinitely for user to\n",
    "# speak\n",
    "m=sr.Microphone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while(1):\n",
    "\n",
    "        # use the microphone as source for input.\n",
    "        with m as source2:\n",
    "            print('say smthing')\n",
    "            # wait for a second to let the recognizer\n",
    "            # adjust the energy threshold based on\n",
    "            # the surrounding noise level\n",
    "            t=time.time()\n",
    "            r.adjust_for_ambient_noise(source2, duration=0.8)\n",
    "            #listens for the user's input\n",
    "#             audio2 = r.listen(source2,)\n",
    "            audio2=r.record(source2, duration=4)\n",
    "#             print(audio2)\n",
    "            # Using google to recognize audio\n",
    "            # try:\n",
    "            MyText = r.recognize_google(audio2)\n",
    "            # except:\n",
    "            #     print('repeat')\n",
    "            #     audio2=r.record(source2, duration=3)\n",
    "            #     MyText = r.(audio2)\n",
    "            \n",
    "\n",
    "\n",
    "            MyText = MyText.lower()\n",
    "            give_com(MyText)\n",
    "            # MyText = r.recognize_google(audio2)\n",
    "            # give_com(MyText.lower())\n",
    "\n",
    "\n",
    "            time.sleep(1.5)\n",
    "            print(\"Did you say \"+MyText)\n",
    "            print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_single():\n",
    "        with m as source2:\n",
    "                print('say smthing')\n",
    "                # wait for a second to let the recognizer\n",
    "                # adjust the energy threshold based on\n",
    "                # the surrounding noise level\n",
    "                # t=time.time()\n",
    "                r.adjust_for_ambient_noise(source2, duration=0.3)\n",
    "                #listens for the user's input\n",
    "        #             audio2 = r.listen(source2,)\n",
    "                audio2=r.record(source2, duration=3)\n",
    "        #             print(audio2)\n",
    "                # Using google to recognize audio\n",
    "                try:\n",
    "                        MyText = r.recognize_google(audio2)\n",
    "                except:\n",
    "                    print('repeat')\n",
    "                    audio2=r.record(source2, duration=3)\n",
    "                    MyText = r.recognize_google(audio2)\n",
    "                    \n",
    "\n",
    "                MyText = MyText.lower()\n",
    "                give_com(MyText)\n",
    "        # MyText = r.recognize_google(audio2)\n",
    "        # give_com(MyText.lower())\n",
    "\n",
    "\n",
    "        # time.sleep(1.5)\n",
    "        print(\"Did you say \"+MyText)\n",
    "        # print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say smthing\n",
      "Did you say left\n"
     ]
    }
   ],
   "source": [
    "# give_single()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Graduating Senior\\Spring\\Embedded\\Project\\ES project.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 99>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Graduating%20Senior/Spring/Embedded/Project/ES%20project.ipynb#ch0000010?line=97'>98</a>\u001b[0m \u001b[39m# Start streaming from microphone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Graduating%20Senior/Spring/Embedded/Project/ES%20project.ipynb#ch0000010?line=98'>99</a>\u001b[0m \u001b[39mwith\u001b[39;00m sd\u001b[39m.\u001b[39mInputStream(channels\u001b[39m=\u001b[39mnum_channels,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Graduating%20Senior/Spring/Embedded/Project/ES%20project.ipynb#ch0000010?line=99'>100</a>\u001b[0m                     samplerate\u001b[39m=\u001b[39msample_rate,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Graduating%20Senior/Spring/Embedded/Project/ES%20project.ipynb#ch0000010?line=100'>101</a>\u001b[0m                     blocksize\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(sample_rate \u001b[39m*\u001b[39m rec_duration),\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Graduating%20Senior/Spring/Embedded/Project/ES%20project.ipynb#ch0000010?line=101'>102</a>\u001b[0m                     callback\u001b[39m=\u001b[39msd_callback):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Graduating%20Senior/Spring/Embedded/Project/ES%20project.ipynb#ch0000010?line=102'>103</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Graduating%20Senior/Spring/Embedded/Project/ES%20project.ipynb#ch0000010?line=103'>104</a>\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import timeit\n",
    "import python_speech_features\n",
    "\n",
    "\n",
    "# Parameters\n",
    "debug_time = 1\n",
    "debug_acc = 0\n",
    "led_pin = 8\n",
    "word_threshold = 0.5\n",
    "rec_duration = 0.5\n",
    "window_stride = 2\n",
    "sample_rate = 48000\n",
    "resample_rate = 8000\n",
    "num_channels = 1\n",
    "num_mfcc = 16\n",
    "model_path = 'D:\\Graduating Senior\\Spring\\Embedded\\Project\\speech_model.h5'\n",
    "model=tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Sliding window\n",
    "window = np.zeros(int(rec_duration * resample_rate) * 2)\n",
    "\n",
    "# GPIO \n",
    "# GPIO.setwarnings(False)\n",
    "# GPIO.setmode(GPIO.BOARD)\n",
    "# GPIO.setup(8, GPIO.OUT, initial=GPIO.LOW)\n",
    "\n",
    "# # Load model (interpreter)\n",
    "# interpreter = Interpreter(model_path)\n",
    "# interpreter.allocate_tensors()\n",
    "# input_details = interpreter.get_input_details()\n",
    "# output_details = interpreter.get_output_details()\n",
    "# print(input_details)\n",
    "\n",
    "# Decimate (filter and downsample)\n",
    "def decimate(signal, old_fs, new_fs):\n",
    "    \n",
    "    # Check to make sure we're downsampling\n",
    "    if new_fs > old_fs:\n",
    "        print(\"Error: target sample rate higher than original\")\n",
    "        return signal, old_fs\n",
    "    \n",
    "    # We can only downsample by an integer factor\n",
    "    dec_factor = old_fs / new_fs\n",
    "    if not dec_factor.is_integer():\n",
    "        print(\"Error: can only decimate by integer factor\")\n",
    "        return signal, old_fs\n",
    "\n",
    "    # Do decimation\n",
    "    resampled_signal = scipy.signal.decimate(signal, int(dec_factor))\n",
    "\n",
    "    return resampled_signal, new_fs\n",
    "\n",
    "def sd_callback(rec, frames, time, status):\n",
    "\n",
    "#     GPIO.output(led_pin, GPIO.LOW)\n",
    "\n",
    "    # Start timing for testing\n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    # Notify if errors\n",
    "    if status:\n",
    "        print('Error:', status)\n",
    "    \n",
    "    # Remove 2nd dimension from recording sample\n",
    "    rec = np.squeeze(rec)\n",
    "    # print(rec)\n",
    "    # Resample\n",
    "    rec, new_fs = decimate(rec, sample_rate, resample_rate)\n",
    "    \n",
    "    # Save recording onto sliding window\n",
    "    window[:len(window)//2] = window[len(window)//2:]\n",
    "    window[len(window)//2:] = rec\n",
    "\n",
    "    # Compute features\n",
    "    mfccs = python_speech_features.base.mfcc(window, \n",
    "                                        samplerate=new_fs,\n",
    "                                        winlen=0.256,\n",
    "                                        winstep=0.050,\n",
    "                                        numcep=num_mfcc,\n",
    "                                        nfilt=26,\n",
    "                                        nfft=2048,\n",
    "                                        preemph=0.0,\n",
    "                                        ceplifter=0,\n",
    "                                        appendEnergy=False,\n",
    "                                        winfunc=np.hanning)\n",
    "    mfccs = mfccs.transpose()\n",
    "#     print(mfccs)\n",
    "    # for m in mfccs:\n",
    "    #     plt.scatter(list(range(len(m))),m)\n",
    "    #     plt.show()\n",
    "    # Make prediction from model\n",
    "    in_tensor = np.float32(mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1], 1))\n",
    "    preds=model.predict(in_tensor,verbose=False)\n",
    "    print(np.argmax(preds.ravel()))\n",
    "# Start streaming from microphone\n",
    "with sd.InputStream(channels=num_channels,\n",
    "                    samplerate=sample_rate,\n",
    "                    blocksize=int(sample_rate * rec_duration),\n",
    "                    callback=sd_callback):\n",
    "    while True:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path='left_1.wav'\n",
    "sr,wv=read(f_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   1   1 ... 330 324 321]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "rec=wv\n",
    "rec = np.squeeze(rec)\n",
    "print(rec)\n",
    "# Resample\n",
    "rec, new_fs = decimate(rec, 16000, 4000)\n",
    "\n",
    "# Save recording onto sliding window\n",
    "window[:len(window)//2] = window[len(window)//2:]\n",
    "window[len(window)//2:] = rec\n",
    "\n",
    "# Compute features\n",
    "mfccs = python_speech_features.base.mfcc(window, \n",
    "                                    samplerate=new_fs,\n",
    "                                    winlen=0.256,\n",
    "                                    winstep=0.050,\n",
    "                                    numcep=num_mfcc,\n",
    "                                    nfilt=26,\n",
    "                                    nfft=2048,\n",
    "                                    preemph=0.0,\n",
    "                                    ceplifter=0,\n",
    "                                    appendEnergy=False,\n",
    "                                    winfunc=np.hanning)\n",
    "mfccs = mfccs.transpose()\n",
    "#     print(mfccs)\n",
    "# for m in mfccs:\n",
    "#     plt.scatter(list(range(len(m))),m)\n",
    "#     plt.show()\n",
    "# Make prediction from model\n",
    "in_tensor = np.float32(mfccs.reshape(1, mfccs.shape[0], mfccs.shape[1], 1))\n",
    "preds=model.predict(in_tensor,verbose=False)\n",
    "print(np.argmax(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[2].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f711b97d69af3292c95e40daadb50e3bd1d627d46b59dcab703b63509b761170"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ES_')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
